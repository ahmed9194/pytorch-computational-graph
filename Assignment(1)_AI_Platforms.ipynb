{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomNeuralNet, self).__init__()\n",
        "\n",
        "        # Layer 1 parameters (3 neurons)\n",
        "        self.w00 = nn.Parameter(torch.tensor(0.5))\n",
        "        self.b00 = nn.Parameter(torch.tensor(0.1))\n",
        "        self.w01 = nn.Parameter(torch.tensor(-0.3))\n",
        "        self.b01 = nn.Parameter(torch.tensor(0.2))\n",
        "        self.w02 = nn.Parameter(torch.tensor(0.8))\n",
        "        self.b02 = nn.Parameter(torch.tensor(-0.1))\n",
        "\n",
        "        # Layer 2 parameters (2 neurons)\n",
        "        self.w10 = nn.Parameter(torch.tensor([0.6, -0.4, 0.9]))\n",
        "        self.b10 = nn.Parameter(torch.tensor(0.3))\n",
        "        self.w11 = nn.Parameter(torch.tensor([-0.2, 0.7, 0.1]))\n",
        "        self.b11 = nn.Parameter(torch.tensor(-0.2))\n",
        "\n",
        "        # Output layer parameters (1 neuron)\n",
        "        self.w20 = nn.Parameter(torch.tensor(1.5))\n",
        "        self.b20 = nn.Parameter(torch.tensor(0.5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\" CUSTOM NEURAL NETWORK FORWARD PASS\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Input x: {x.item()}\")\n",
        "\n",
        "        print(\"\\n--- LAYER 1: 3 Neurons + ReLU ---\")\n",
        "\n",
        "        z00 = self.w00 * x + self.b00\n",
        "        z01 = self.w01 * x + self.b01\n",
        "        z02 = self.w02 * x + self.b02\n",
        "\n",
        "        print(f\"Pre-activation:\")\n",
        "        print(f\"  z00 = w00*x + b00 = {z00.item():.4f}\")\n",
        "        print(f\"  z01 = w01*x + b01 = {z01.item():.4f}\")\n",
        "        print(f\"  z02 = w02*x + b02 = {z02.item():.4f}\")\n",
        "\n",
        "        a00 = torch.relu(z00)\n",
        "        a01 = torch.relu(z01)\n",
        "        a02 = torch.relu(z02)\n",
        "\n",
        "        print(f\"Post-ReLU:\")\n",
        "        print(f\"  a00 = relu(z00) = {a00.item():.4f}\")\n",
        "        print(f\"  a01 = relu(z01) = {a01.item():.4f}\")\n",
        "        print(f\"  a02 = relu(z02) = {a02.item():.4f}\")\n",
        "\n",
        "        print(\"\\n--- LAYER 2: 2 Neurons + Sigmoid ---\")\n",
        "\n",
        "        z10 = self.w10[0] * a00 + self.w10[1] * a01 + self.w10[2] * a02 + self.b10\n",
        "        z11 = self.w11[0] * a00 + self.w11[1] * a01 + self.w11[2] * a02 + self.b11\n",
        "\n",
        "        print(f\"Pre-activation:\")\n",
        "        print(f\"  z10 = w10[0]*a00 + w10[1]*a01 + w10[2]*a02 + b10 = {z10.item():.4f}\")\n",
        "        print(f\"  z11 = w11[0]*a00 + w11[1]*a01 + w11[2]*a02 + b11 = {z11.item():.4f}\")\n",
        "\n",
        "        a10 = torch.sigmoid(z10)\n",
        "        a11 = torch.sigmoid(z11)\n",
        "\n",
        "        print(f\"Post-Sigmoid:\")\n",
        "        print(f\"  a10 = sigmoid(z10) = {a10.item():.4f}\")\n",
        "        print(f\"  a11 = sigmoid(z11) = {a11.item():.4f}\")\n",
        "\n",
        "        print(\"\\n--- COMBINE + Tanh ---\")\n",
        "\n",
        "        combined = a10 + a11\n",
        "        print(f\"Combined outputs: a10 + a11 = {combined.item():.4f}\")\n",
        "\n",
        "        tanh_output = torch.tanh(combined)\n",
        "        print(f\"After Tanh: tanh(combined) = {tanh_output.item():.4f}\")\n",
        "\n",
        "        print(\"\\n--- OUTPUT LAYER: Linear ---\")\n",
        "\n",
        "        output = self.w20 * tanh_output + self.b20\n",
        "        print(f\"Final output: w20*tanh_output + b20 = {output.item():.4f}\")\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "aiWICCmfEHQS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_parameters(model):\n",
        "    print(\"\\n MODEL PARAMETERS:\")\n",
        "    print(f\"Layer 1 - w00: {model.w00.item():.4f}, b00: {model.b00.item():.4f}\")\n",
        "    print(f\"         w01: {model.w01.item():.4f}, b01: {model.b01.item():.4f}\")\n",
        "    print(f\"         w02: {model.w02.item():.4f}, b02: {model.b02.item():.4f}\")\n",
        "    print(f\"Layer 2 - w10: {model.w10.detach().numpy()}, b10: {model.b10.item():.4f}\")\n",
        "    print(f\"         w11: {model.w11.detach().numpy()}, b11: {model.b11.item():.4f}\")\n",
        "    print(f\"Output  - w20: {model.w20.item():.4f}, b20: {model.b20.item():.4f}\")\n",
        "\n",
        "def compute_gradients(model, x, output):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" BACKWARD PASS: Computing Gradients\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if x.grad is not None:\n",
        "        x.grad.zero_()\n",
        "    model.zero_grad()\n",
        "\n",
        "    output.backward()\n",
        "\n",
        "    print(\" GRADIENTS:\")\n",
        "    print(f\"∂output/∂x: {x.grad.item():.6f}\")\n",
        "\n",
        "    print(f\"\\nLayer 1 gradients:\")\n",
        "    print(f\"  ∂output/∂w00: {model.w00.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂b00: {model.b00.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂w01: {model.w01.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂b01: {model.b01.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂w02: {model.w02.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂b02: {model.b02.grad.item():.6f}\")\n",
        "\n",
        "    print(f\"\\nLayer 2 gradients:\")\n",
        "    print(f\"  ∂output/∂w10: {model.w10.grad}\")\n",
        "    print(f\"  ∂output/∂b10: {model.b10.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂w11: {model.w11.grad}\")\n",
        "    print(f\"  ∂output/∂b11: {model.b11.grad.item():.6f}\")\n",
        "\n",
        "    print(f\"\\nOutput layer gradients:\")\n",
        "    print(f\"  ∂output/∂w20: {model.w20.grad.item():.6f}\")\n",
        "    print(f\"  ∂output/∂b20: {model.b20.grad.item():.6f}\")"
      ],
      "metadata": {
        "id": "l6VOqm-UEMJM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\" INITIALIZING CUSTOM NEURAL NETWORK\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    model = CustomNeuralNet()\n",
        "    print_parameters(model)\n",
        "\n",
        "    x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" FORWARD PASS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    output = model(x)\n",
        "    compute_gradients(model, x, output)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" EXECUTION COMPLETED\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Final output: {output.item():.6f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwF3pry9EaFn",
        "outputId": "436a7ef2-d1f2-406f-addb-3e44d915bca6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " INITIALIZING CUSTOM NEURAL NETWORK\n",
            "==================================================\n",
            "\n",
            " MODEL PARAMETERS:\n",
            "Layer 1 - w00: 0.5000, b00: 0.1000\n",
            "         w01: -0.3000, b01: 0.2000\n",
            "         w02: 0.8000, b02: -0.1000\n",
            "Layer 2 - w10: [ 0.6 -0.4  0.9], b10: 0.3000\n",
            "         w11: [-0.2  0.7  0.1], b11: -0.2000\n",
            "Output  - w20: 1.5000, b20: 0.5000\n",
            "\n",
            "==================================================\n",
            " FORWARD PASS\n",
            "==================================================\n",
            " CUSTOM NEURAL NETWORK FORWARD PASS\n",
            "==================================================\n",
            "Input x: 2.0\n",
            "\n",
            "--- LAYER 1: 3 Neurons + ReLU ---\n",
            "Pre-activation:\n",
            "  z00 = w00*x + b00 = 1.1000\n",
            "  z01 = w01*x + b01 = -0.4000\n",
            "  z02 = w02*x + b02 = 1.5000\n",
            "Post-ReLU:\n",
            "  a00 = relu(z00) = 1.1000\n",
            "  a01 = relu(z01) = 0.0000\n",
            "  a02 = relu(z02) = 1.5000\n",
            "\n",
            "--- LAYER 2: 2 Neurons + Sigmoid ---\n",
            "Pre-activation:\n",
            "  z10 = w10[0]*a00 + w10[1]*a01 + w10[2]*a02 + b10 = 2.3100\n",
            "  z11 = w11[0]*a00 + w11[1]*a01 + w11[2]*a02 + b11 = -0.2700\n",
            "Post-Sigmoid:\n",
            "  a10 = sigmoid(z10) = 0.9097\n",
            "  a11 = sigmoid(z11) = 0.4329\n",
            "\n",
            "--- COMBINE + Tanh ---\n",
            "Combined outputs: a10 + a11 = 1.3426\n",
            "After Tanh: tanh(combined) = 0.8723\n",
            "\n",
            "--- OUTPUT LAYER: Linear ---\n",
            "Final output: w20*tanh_output + b20 = 1.8084\n",
            "\n",
            "==================================================\n",
            " BACKWARD PASS: Computing Gradients\n",
            "==================================================\n",
            " GRADIENTS:\n",
            "∂output/∂x: 0.028289\n",
            "\n",
            "Layer 1 gradients:\n",
            "  ∂output/∂w00: 0.000134\n",
            "  ∂output/∂b00: 0.000067\n",
            "  ∂output/∂w01: 0.000000\n",
            "  ∂output/∂b01: 0.000000\n",
            "  ∂output/∂w02: 0.070639\n",
            "  ∂output/∂b02: 0.035319\n",
            "\n",
            "Layer 2 gradients:\n",
            "  ∂output/∂w10: tensor([0.0324, 0.0000, 0.0442])\n",
            "  ∂output/∂b10: 0.029461\n",
            "  ∂output/∂w11: tensor([0.0969, 0.0000, 0.1321])\n",
            "  ∂output/∂b11: 0.088047\n",
            "\n",
            "Output layer gradients:\n",
            "  ∂output/∂w20: 0.872297\n",
            "  ∂output/∂b20: 1.000000\n",
            "\n",
            "==================================================\n",
            " EXECUTION COMPLETED\n",
            "==================================================\n",
            "Final output: 1.808446\n"
          ]
        }
      ]
    }
  ]
}